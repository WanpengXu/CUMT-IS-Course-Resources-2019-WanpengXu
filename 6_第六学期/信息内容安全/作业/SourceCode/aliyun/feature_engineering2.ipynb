{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../security_data/'\n",
    "train = pd.read_csv(path + 'security_train.csv')\n",
    "test  = pd.read_csv(path + 'security_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存管理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "\n",
    "class _Data_Preprocess:\n",
    "    def __init__(self):\n",
    "        self.int8_max = np.iinfo(np.int8).max\n",
    "        self.int8_min = np.iinfo(np.int8).min\n",
    "\n",
    "        self.int16_max = np.iinfo(np.int16).max\n",
    "        self.int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "        self.int32_max = np.iinfo(np.int32).max\n",
    "        self.int32_min = np.iinfo(np.int32).min\n",
    "\n",
    "        self.int64_max = np.iinfo(np.int64).max\n",
    "        self.int64_min = np.iinfo(np.int64).min\n",
    "\n",
    "        self.float16_max = np.finfo(np.float16).max\n",
    "        self.float16_min = np.finfo(np.float16).min\n",
    "\n",
    "        self.float32_max = np.finfo(np.float32).max\n",
    "        self.float32_min = np.finfo(np.float32).min\n",
    "\n",
    "        self.float64_max = np.finfo(np.float64).max\n",
    "        self.float64_min = np.finfo(np.float64).min\n",
    "\n",
    "    def _get_type(self, min_val, max_val, types):\n",
    "        if types == 'int':\n",
    "            if max_val <= self.int8_max and min_val >= self.int8_min:\n",
    "                return np.int8\n",
    "            elif max_val <= self.int16_max <= max_val and min_val >= self.int16_min:\n",
    "                return np.int16\n",
    "            elif max_val <= self.int32_max and min_val >= self.int32_min:\n",
    "                return np.int32\n",
    "            return None\n",
    "\n",
    "        elif types == 'float':\n",
    "            if max_val <= self.float16_max and min_val >= self.float16_min:\n",
    "                return np.float16\n",
    "            if max_val <= self.float32_max and min_val >= self.float32_min:\n",
    "                return np.float32\n",
    "            if max_val <= self.float64_max and min_val >= self.float64_min:\n",
    "                return np.float64\n",
    "            return None\n",
    "\n",
    "    def _memory_process(self, df):\n",
    "        init_memory = df.memory_usage().sum() / 1024 ** 2 / 1024\n",
    "        print('Original data occupies {} GB memory.'.format(init_memory))\n",
    "        df_cols = df.columns\n",
    "\n",
    "          \n",
    "        for col in tqdm_notebook(df_cols):\n",
    "            try:\n",
    "                if 'float' in str(df[col].dtypes):\n",
    "                    max_val = df[col].max()\n",
    "                    min_val = df[col].min()\n",
    "                    trans_types = self._get_type(min_val, max_val, 'float')\n",
    "                    if trans_types is not None:\n",
    "                        df[col] = df[col].astype(trans_types)\n",
    "                elif 'int' in str(df[col].dtypes):\n",
    "                    max_val = df[col].max()\n",
    "                    min_val = df[col].min()\n",
    "                    trans_types = self._get_type(min_val, max_val, 'int')\n",
    "                    if trans_types is not None:\n",
    "                        df[col] = df[col].astype(trans_types)\n",
    "            except:\n",
    "                print(' Can not do any process for column, {}.'.format(col)) \n",
    "        afterprocess_memory = df.memory_usage().sum() / 1024 ** 2 / 1024\n",
    "        print('After processing, the data occupies {} GB memory.'.format(afterprocess_memory))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_process = _Data_Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sts_features(df):\n",
    "    simple_fea = pd.DataFrame()\n",
    "    simple_fea['file_id'] = df['file_id'].unique()\n",
    "    simple_fea = simple_fea.sort_values('file_id')\n",
    "     \n",
    "    df_grp = df.groupby('file_id')\n",
    "    simple_fea['file_id_api_count'] = df_grp['api'].count().values\n",
    "    simple_fea['file_id_api_nunique'] = df_grp['api'].nunique().values\n",
    "    \n",
    "    simple_fea['file_id_tid_count'] = df_grp['tid'].count().values\n",
    "    simple_fea['file_id_tid_nunique'] = df_grp['tid'].nunique().values\n",
    "    \n",
    "    simple_fea['file_id_index_count'] = df_grp['index'].count().values\n",
    "    simple_fea['file_id_index_nunique'] = df_grp['index'].nunique().values\n",
    "    \n",
    "    return simple_fea\n",
    "\n",
    "def simple_numerical_sts_features(df):\n",
    "    simple_numerical_fea = pd.DataFrame()\n",
    "    simple_numerical_fea['file_id'] = df['file_id'].unique()\n",
    "    simple_numerical_fea = simple_numerical_fea.sort_values('file_id')\n",
    "     \n",
    "    df_grp = df.groupby('file_id')\n",
    "    \n",
    "    simple_numerical_fea['file_id_tid_mean'] = df_grp['tid'].mean().values\n",
    "    simple_numerical_fea['file_id_tid_min'] = df_grp['tid'].min().values\n",
    "    simple_numerical_fea['file_id_tid_std'] = df_grp['tid'].std().values\n",
    "    simple_numerical_fea['file_id_tid_max'] = df_grp['tid'].max().values\n",
    "    \n",
    "    \n",
    "    simple_numerical_fea['file_id_index_mean'] = df_grp['index'].mean().values\n",
    "    simple_numerical_fea['file_id_index_min'] = df_grp['index'].min().values\n",
    "    simple_numerical_fea['file_id_index_std'] = df_grp['index'].std().values\n",
    "    simple_numerical_fea['file_id_index_max'] = df_grp['index'].max().values\n",
    "    \n",
    "    return simple_numerical_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train_fea1 = simple_sts_features(train)\n",
    "simple_test_fea1 = simple_sts_features(test)\n",
    "\n",
    "simple_train_fea2 = simple_numerical_sts_features(train)\n",
    "simple_test_fea2 = simple_numerical_sts_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_pivot_count_features(df):\n",
    "    tmp = df.groupby(['file_id','api'])['tid'].count().to_frame('api_tid_count').reset_index()\n",
    "    tmp_pivot = pd.pivot_table(data=tmp,index = 'file_id',columns='api',values='api_tid_count',fill_value=0)\n",
    "    tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "    tmp_pivot.reset_index(inplace = True)\n",
    "    tmp_pivot = memory_process._memory_process(tmp_pivot)\n",
    "    return tmp_pivot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_pivot_nunique_features(df):\n",
    "    tmp = df.groupby(['file_id','api'])['tid'].nunique().to_frame('api_tid_nunique').reset_index()\n",
    "    tmp_pivot = pd.pivot_table(data=tmp,index = 'file_id',columns='api',values='api_tid_nunique',fill_value=0)\n",
    "    tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "    tmp_pivot.reset_index(inplace = True)\n",
    "    tmp_pivot = memory_process._memory_process(tmp_pivot)\n",
    "    return tmp_pivot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train_fea3 = api_pivot_count_features(train)\n",
    "simple_test_fea3 = api_pivot_count_features(test)\n",
    "\n",
    "simple_train_fea4 = api_pivot_nunique_features(train)\n",
    "simple_test_fea4 = api_pivot_nunique_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train[['file_id','label']].drop_duplicates(subset = ['file_id','label'], keep = 'first')\n",
    "test_submit = test[['file_id']].drop_duplicates(subset = ['file_id'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_label.merge(simple_train_fea1, on ='file_id', how='left')\n",
    "train_data = train_data.merge(simple_train_fea2, on ='file_id', how='left')\n",
    "train_data = train_data.merge(simple_train_fea3, on ='file_id', how='left')\n",
    "train_data = train_data.merge(simple_train_fea4, on ='file_id', how='left')\n",
    "\n",
    "test_submit = test_submit.merge(simple_test_fea1, on ='file_id', how='left')\n",
    "test_submit = test_submit.merge(simple_test_fea2, on ='file_id', how='left')\n",
    "test_submit = test_submit.merge(simple_test_fea3, on ='file_id', how='left')\n",
    "test_submit = test_submit.merge(simple_test_fea4, on ='file_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 评估指标构建\n",
    "def lgb_logloss(preds,data):\n",
    "    labels_ = data.get_label()             \n",
    "    classes_ = np.unique(labels_) \n",
    "    preds_prob = []\n",
    "    for i in range(len(classes_)):\n",
    "        preds_prob.append(preds[i*len(labels_):(i+1) * len(labels_)] )\n",
    "        \n",
    "    preds_prob_ = np.vstack(preds_prob) \n",
    "    \n",
    "    loss = []\n",
    "    for i in range(preds_prob_.shape[1]):     \n",
    "        sum_ = 0\n",
    "        for j in range(preds_prob_.shape[0]): \n",
    "            pred = preds_prob_[j,i]           \n",
    "            if  j == labels_[i]:\n",
    "                sum_ += np.log(pred)\n",
    "            else:\n",
    "                sum_ += np.log(1 - pred)\n",
    "        loss.append(sum_)       \n",
    "    return 'loss is: ',-1 * (np.sum(loss) / preds_prob_.shape[1]),False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [col for col in train_data.columns if col not in ['label','file_id']]\n",
    "train_label = 'label'\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "params = {\n",
    "        'task':'train', \n",
    "        'num_leaves': 255,\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 8,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5, \n",
    "        'max_bin':128,\n",
    "        'random_state':100\n",
    "    }   \n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "\n",
    "predict_res = 0\n",
    "models = []\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][train_features], label=train_data.iloc[trn_idx][train_label].values)\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][train_features], label=train_data.iloc[val_idx][train_label].values) \n",
    "    \n",
    "    clf = lgb.train(params, trn_data, num_boost_round=2000,valid_sets=[trn_data,val_data], verbose_eval=50, early_stopping_rounds=100, feval=lgb_logloss) \n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "sns.heatmap(train_data.iloc[:10000, 1:21].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 特征重要性分析\n",
    "feature_importance = pd.DataFrame()\n",
    "feature_importance['fea_name'] = train_features\n",
    "feature_importance['fea_imp'] = clf.feature_importance()\n",
    "feature_importance = feature_importance.sort_values('fea_imp',ascending = False)\n",
    "feature_importance.sort_values('fea_imp',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10,])\n",
    "sns.barplot(x=feature_importance.iloc[:10]['fea_name'], y=feature_importance.iloc[:10]['fea_imp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 10,])\n",
    "sns.barplot(x = feature_importance['fea_name'], y = feature_importance['fea_imp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_tree(clf, tree_index=1, figsize=(20, 8), show_info=['split_gain'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res = 0\n",
    "flod = 5\n",
    "for model in models:\n",
    "    pred_res += model.predict(test_submit[train_features]) * 1.0 / flod\n",
    "\n",
    "test_submit['prob0'] = 0\n",
    "test_submit['prob1'] = 0\n",
    "test_submit['prob2'] = 0\n",
    "test_submit['prob3'] = 0\n",
    "test_submit['prob4'] = 0\n",
    "test_submit['prob5'] = 0\n",
    "test_submit['prob6'] = 0\n",
    "test_submit['prob7'] = 0\n",
    "\n",
    "test_submit[['prob0','prob1','prob2','prob3','prob4','prob5','prob6','prob7']] = pred_res\n",
    "test_submit[['file_id','prob0','prob1','prob2','prob3','prob4','prob5','prob6','prob7']].to_csv('baseline2.csv',index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4054109a07366a15e0872477e08b55b91c055879d5d14672549f1c0c69ac48ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
